<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Components · Ju.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="Ju.jl logo"/></a><h1>Ju.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><a class="toctext" href="../interfaces/">Interfaces</a></li><li class="current"><a class="toctext" href>Components</a><ul class="internal"><li><a class="toctext" href="#Environments-1">Environments</a></li><li><a class="toctext" href="#Spaces-1">Spaces</a></li><li><a class="toctext" href="#Agents-1">Agents</a></li><li><a class="toctext" href="#Buffers-1">Buffers</a></li><li><a class="toctext" href="#Learners-1">Learners</a></li><li><a class="toctext" href="#Approximators-1">Approximators</a></li><li><a class="toctext" href="#Environment-Models-1">Environment Models</a></li><li><a class="toctext" href="#Policies-1">Policies</a></li><li><a class="toctext" href="#Action-Selectors-1">Action Selectors</a></li></ul></li><li><a class="toctext" href="../utilities/">Utilities</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Components</a></li></ul><a class="edit-page" href="https://github.com/Ju-jl/Ju.jl/blob/master/docs/src/components.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Components</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Components-1" href="#Components-1">Components</a></h1><p>This package has implemented some basic components which inherit from the abstract types defined at <a href="../interfaces/#interfaces_section-1"><code>Interfaces</code></a>.</p><ul><li><a href="#Components-1">Components</a></li><ul><li><a href="#Environments-1">Environments</a></li><li><a href="#Spaces-1">Spaces</a></li><li><a href="#Agents-1">Agents</a></li><li><a href="#Buffers-1">Buffers</a></li><li><a href="#Learners-1">Learners</a></li><li><a href="#Approximators-1">Approximators</a></li><li><a href="#Environment-Models-1">Environment Models</a></li><li><a href="#Policies-1">Policies</a></li><li><a href="#Action-Selectors-1">Action Selectors</a></li></ul></ul><h2><a class="nav-anchor" id="Environments-1" href="#Environments-1">Environments</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.SimpleRandomWalkEnv" href="#Ju.SimpleRandomWalkEnv"><code>Ju.SimpleRandomWalkEnv</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SimpleRandomWalkEnv</code></pre><p>A simple random walk environment for tutorial.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; env = SimpleRandomWalkEnv()
SimpleRandomWalkEnv(7, 3, 3, [-1, 1])

julia&gt; env(1)
(observation = 2, reward = 0.0, isdone = false)

julia&gt; reset!(env)
(observation = 3, isdone = false)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/environments/simple_random_walk_env.jl#L3-L19">source</a></section><h2><a class="nav-anchor" id="Spaces-1" href="#Spaces-1">Spaces</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.ContinuousSpace" href="#Ju.ContinuousSpace"><code>Ju.ContinuousSpace</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct ContinuousSpace{T&lt;:Number} &lt;: AbstractContinuousSpace
    low::T
    high::T
end</code></pre><p>The lower bound and upper bound are specifed by <code>low</code> and <code>high</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/spaces/continuous_space.jl#L1-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DiscreteSpace" href="#Ju.DiscreteSpace"><code>Ju.DiscreteSpace</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct DiscreteSpace &lt;: AbstractDiscreteSpace
    n::Int
end</code></pre><p>The elements in a <code>DiscreteSpace</code> is <code>1:n</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/spaces/discrete_space.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.MultiContinuousSpace" href="#Ju.MultiContinuousSpace"><code>Ju.MultiContinuousSpace</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MultiContinuousSpace(low::Number, high::Number, size::Tuple{Vararg{Int}})
MultiContinuousSpace(low::Array{&lt;:Number}, high::Array{&lt;:Number})</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">MultiContinuousSpace(-1, 1, (2,3))
MultiContinuousSpace([0, 0, 0], [1, 2, 3])</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/spaces/multi_continuous_space.jl#L1-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.MultiDiscreteSpace" href="#Ju.MultiDiscreteSpace"><code>Ju.MultiDiscreteSpace</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct MultiDiscreteSpace{N} &lt;:AbstractDiscreteSpace
    counts::Array{Int, N}
end</code></pre><p>The element in <code>MultiDiscreteSpace{N}</code> is a multi-dimension array.  The number of each dimension is specified by <code>counts</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/spaces/multi_discrete_space.jl#L1-L8">source</a></section><h2><a class="nav-anchor" id="Agents-1" href="#Agents-1">Agents</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.Agent" href="#Ju.Agent"><code>Ju.Agent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Agent{Tl&lt;:AbstractLearner, Tb&lt;:AbstractTurnBuffer, Tpp&lt;:Function} &lt;: AbstractAgent 
Agent(learner::Tl, buffer::Tb, preprocessor::Tpp=identity, role=:anonymous) where {Tl&lt;:AbstractLearner, Tb&lt;:AbstractTurnBuffer, Tpp&lt;:Function}</code></pre><p>A <code>preprocessor</code> is just a normal function. It transforms the <strong>observation</strong> from an environment to the internal <strong>state</strong>, which is then stored in the <code>buffer</code>. <code>role</code> is a <code>Symbol</code>. Usually it is used in multi-agents environment to distinction different agents.</p><p>See also: <a href="../interfaces/#Ju.AbstractLearner"><code>AbstractLearner</code></a>, <a href="../interfaces/#Ju.AbstractTurnBuffer"><code>AbstractTurnBuffer</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/agents/agent.jl#L1-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.Agent-Tuple{Any}" href="#Ju.Agent-Tuple{Any}"><code>Ju.Agent</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">(agent::Agent)(obs)</code></pre><p>Take in an <code>obs</code> from environment and use <code>agent.preprocessor</code> to transform it into an internal <code>state</code>. Then use <code>agent.learner</code> to get the <code>action</code>. Return a pair of <code>state =&gt; action</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/agents/agent.jl#L49-L55">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DynaAgent" href="#Ju.DynaAgent"><code>Ju.DynaAgent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DynaAgent{Tl&lt;:AbstractLearner, Tb&lt;:AbstractTurnBuffer, Tm&lt;:AbstractEnvironmentModel, Tpp&lt;:Function} &lt;: AbstractAgent
DynaAgent(learner::Tl, buffer::Tb, model::Tm, nsteps::Int=0, preprocessor::Tpp=identity, role=:anonymous) where {Tl, Tb, Tm, Tpp}</code></pre><p>See more details at Section (8.2) on Page 162 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p><p>See also: <a href="#Ju.Agent"><code>Agent</code></a>, <a href="../interfaces/#Ju.AbstractLearner"><code>AbstractLearner</code></a>, <a href="../interfaces/#Ju.AbstractTurnBuffer"><code>AbstractTurnBuffer</code></a>, <a href="../interfaces/#Ju.AbstractEnvironmentModel"><code>AbstractEnvironmentModel</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/agents/dyna_agent.jl#L1-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DynaAgent-Tuple{Any}" href="#Ju.DynaAgent-Tuple{Any}"><code>Ju.DynaAgent</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">(agent::DynaAgent)(obs)</code></pre><p>Take in an <code>obs</code> from environment and use <code>agent.preprocessor</code> to transform it into an internal <code>state</code>. Then use <code>agent.learner</code> to get the <code>action</code>. Return a pair of <code>state =&gt; action</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/agents/dyna_agent.jl#L35-L41">source</a></section><h2><a class="nav-anchor" id="Buffers-1" href="#Buffers-1">Buffers</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.CircularSARDBuffer" href="#Ju.CircularSARDBuffer"><code>Ju.CircularSARDBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>CircularSARDBuffer</code> is just an alias for <code>CircularTurnBuffer{(:state, :action, :reward, :isdone)}</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/circular_turn_buffer.jl#L25">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.CircularSARDBuffer-Tuple{Any}" href="#Ju.CircularSARDBuffer-Tuple{Any}"><code>Ju.CircularSARDBuffer</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">CircularSARDBuffer(capacity; state_type::Type=Int, action_type::Type=Int, state_size=(), action_size=())</code></pre><p><code>capacity</code> specifies how many latest turns the buffer will store at most. </p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>Note that, the length of state and action is 1 step longer in oder to store the state and action in the next step. This is the supposed behavior of <strong>SARD</strong> buffers.</p></div></div></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/circular_turn_buffer.jl#L28-L35">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.CircularSARDSABuffer" href="#Ju.CircularSARDSABuffer"><code>Ju.CircularSARDSABuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>CircularSARDSABuffer</code> is just an alias for <code>CircularTurnBuffer{(:state, :action, :reward, :isdone, :nextstate, :nextaction)}</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/circular_turn_buffer.jl#L95">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.CircularSARDSBuffer" href="#Ju.CircularSARDSBuffer"><code>Ju.CircularSARDSBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>CircularSARDSBuffer</code> is just an alias for <code>CircularTurnBuffer{(:state, :action, :reward, :isdone, :nextstate)}</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/circular_turn_buffer.jl#L65">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.CircularTurnBuffer" href="#Ju.CircularTurnBuffer"><code>Ju.CircularTurnBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">CircularTurnBuffer{names, types, Tbs} &lt;: AbstractTurnBuffer{names, types}
CircularTurnBuffer{names, types}(capacities::NTuple{N, Int}, sizes::NTuple{N, NTuple{M, Int} where M}) where {names, types, N}</code></pre><p>Using <code>CircularArrayBuffer</code> to store each element specified in <code>names</code> and <code>types</code>.. The memory of the buffer will be pre-allocated. In RL problems, three of the most common <code>CircularArrayBuffer</code> based buffers are: <a href="#Ju.CircularSARDBuffer"><code>CircularSARDBuffer</code></a>, <a href="#Ju.CircularSARDSBuffer"><code>CircularSARDSBuffer</code></a>, <a href="#Ju.CircularSARDSABuffer"><code>CircularSARDSABuffer</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/circular_turn_buffer.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpisodeSARDBuffer" href="#Ju.EpisodeSARDBuffer"><code>Ju.EpisodeSARDBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>EpisodeSARDBuffer</code> is just an alias for <code>EpisodeTurnBuffer{(:state, :action, :reward, :isdone)}</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/episode_turn_buffer.jl#L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpisodeSARDSABuffer" href="#Ju.EpisodeSARDSABuffer"><code>Ju.EpisodeSARDSABuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>EpisodeSARDSABuffer</code> is just an alias for <code>EpisodeTurnBuffer{(:state, :action, :reward, :isdone, :nextstate, :nextaction)}</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/episode_turn_buffer.jl#L91">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpisodeSARDSBuffer" href="#Ju.EpisodeSARDSBuffer"><code>Ju.EpisodeSARDSBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>EpisodeSARDSBuffer</code> is just an alias for <code>EpisodeTurnBuffer{(:state, :action, :reward, :isdone, :nextstate)}</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/episode_turn_buffer.jl#L71">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpisodeTurnBuffer" href="#Ju.EpisodeTurnBuffer"><code>Ju.EpisodeTurnBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">EpisodeTurnBuffer{names, types, Tbs} &lt;: AbstractTurnBuffer{names, types}
EpisodeTurnBuffer{names, types}() where {names, types}</code></pre><p>Using a Vector to store each element specified by <code>names</code> and <code>types</code>.</p><p>See also: <a href="#Ju.EpisodeSARDBuffer"><code>EpisodeSARDBuffer</code></a>, <a href="#Ju.EpisodeSARDSBuffer"><code>EpisodeSARDSBuffer</code></a>, <a href="#Ju.EpisodeSARDSABuffer"><code>EpisodeSARDSABuffer</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/buffers/episode_turn_buffer.jl#L1-L8">source</a></section><h2><a class="nav-anchor" id="Learners-1" href="#Learners-1">Learners</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DifferentialTDLearner" href="#Ju.DifferentialTDLearner"><code>Ju.DifferentialTDLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DifferentialTDLearner(approximator::Tapp, π::Tp, α::Float64, β::Float64, R̄::Float64=0., n::Int=0, method::Symbol=:SARSA) where {Tapp&lt;:AbstractApproximator, Tp&lt;:PolicyOrSelector}= new{Tapp, Tp, method}(approximator, π, α, β, R̄, n)</code></pre><p>See more details at Section (10.3) on Page 251 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/temporal_difference_learner.jl#L294-L298">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DoubleLearner" href="#Ju.DoubleLearner"><code>Ju.DoubleLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct DoubleLearner{Tl &lt;: OffPolicyTDLearner, Ts&lt;:AbstractActionSelector} &lt;: AbstractLearner
    Learner1::Tl
    Learner2::Tl
    selector::Ts
end</code></pre><p>See more details at Section (6.7) on Page 126 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/temporal_difference_learner.jl#L258-L266">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.GradientBanditLearner" href="#Ju.GradientBanditLearner"><code>Ju.GradientBanditLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct GradientBanditLearner{Ts&lt;:AbstractActionSelector, Tb&lt;:Union{Float64, Function}} &lt;: AbstractModelFreeLearner 
    Q::TabularQ 
    selector::Ts
    α::Float64
    baseline::Tb
end</code></pre><p>See more details at Section (2.8) on Page 37 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/gradient_bandit_learner.jl#L3-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.MonteCarloExploringStartLearner" href="#Ju.MonteCarloExploringStartLearner"><code>Ju.MonteCarloExploringStartLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MonteCarloExploringStartLearner(approximator::Tapp, π::Tp, π_start::RandomPolicy, γ::Float64, α::Float64 = 1.0; is_first_visit::Bool = true) where {Tapp,Tp}</code></pre><p>See more details at Section (5.3) on Page 99 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/monte_carlo_learner.jl#L71-L75">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.MonteCarloLearner" href="#Ju.MonteCarloLearner"><code>Ju.MonteCarloLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MonteCarloLearner(approximator::Tapp, π::Tp, γ::Float64=1., α::Float64 = 1.0, first_visit::Bool = true) where {Tapp,Tp}</code></pre><p>See more details at Section (5.1) on Page 92 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/monte_carlo_learner.jl#L4-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.OffPolicyMonteCarloLearner" href="#Ju.OffPolicyMonteCarloLearner"><code>Ju.OffPolicyMonteCarloLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">OffPolicyMonteCarloLearner(approximator::Tapp, π_behavior::Tpb, π_target::Tpt, γ::Float64, α::Float64 = 1.0; isfirstvisit::Bool = true, sampling::Symbol = :OrdinaryImportanceSampling,) where {Tapp,Tpb,Tpt}</code></pre><p>See more details at Section (5.7) on Page 111 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/monte_carlo_learner.jl#L113-L117">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.OffPolicyTDLearner" href="#Ju.OffPolicyTDLearner"><code>Ju.OffPolicyTDLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">OffPolicyTDLearner(approximator::Tapp, π_behavior::Tpb, π_target::Tpt, γ::Float64, α::Float64, n::Int=0, method::Symbol=:SARSA_ImportanceSampling) where {Tapp&lt;:AbstractApproximator, Tpb&lt;:AbstractPolicy, Tpt&lt;:AbstractPolicy}</code></pre><p>See more details at Section (7.3) on Page 148 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/temporal_difference_learner.jl#L154-L158">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.ReinforceBaselineLearner" href="#Ju.ReinforceBaselineLearner"><code>Ju.ReinforceBaselineLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">mutable struct ReinforceBaselineLearner{Tapp&lt;:AbstractApproximator, Tp&lt;:AbstractPolicy}  &lt;: AbstractModelFreeLearner 
    approximator::Tapp
    π::Tp
    αʷ::Float64
    αᶿ::Float64
    γ::Float64
end</code></pre><p>See more details at Section (13.4) on Page 330 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/reinforce_learner.jl#L33-L43">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.ReinforceLearner" href="#Ju.ReinforceLearner"><code>Ju.ReinforceLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct ReinforceLearner{Tp&lt;:AbstractPolicy}  &lt;: AbstractModelFreeLearner 
    π::Tp
    α::Float64
    γ::Float64
end</code></pre><p>See more details at Section (13.3) on Page 326 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/reinforce_learner.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TDLearner" href="#Ju.TDLearner"><code>Ju.TDLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">TDLearner(approximator::Tapp, π::Tp, γ::Float64, α::Float64, n::Int=0) where {Tapp&lt;:AbstractVApproximator, Tp&lt;:PolicyOrSelector} = new{Tapp, Tp, :SRS}(approximator, π, γ, α, n)
TDLearner(approximator::Tapp, π::Tp, γ::Float64, α::Float64, n::Int=0, method::Symbol=:SARSA) where {Tapp&lt;:AbstractQApproximator, Tp&lt;:PolicyOrSelector}</code></pre><p>See more details at Section (7.1) on Page 142 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/temporal_difference_learner.jl#L3-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TDλReturnLearner" href="#Ju.TDλReturnLearner"><code>Ju.TDλReturnLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct TDλReturnLearner{Tapp &lt;: AbstractApproximator, Tp &lt;: PolicyOrSelector} &lt;: AbstractModelFreeLearner 
    approximator::Tapp
    π::Tp
    γ::Float64
    α::Float64
    λ::Float64
end</code></pre><p>See more details at Section (12.2) on Page 292 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/learners/temporal_difference_learner.jl#L114-L125">source</a></section><h2><a class="nav-anchor" id="Approximators-1" href="#Approximators-1">Approximators</a></h2><h3><a class="nav-anchor" id="Value-Functions-1" href="#Value-Functions-1">Value Functions</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.AggregationV" href="#Ju.AggregationV"><code>Ju.AggregationV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct AggregationV{Tf&lt;:Function} &lt;: AbstractVApproximator{Int}
    table::Vector{Float64}
    f::Tf
end</code></pre><p>Using <code>a.f</code> to map a state <code>s</code> into an <code>Int</code>, then use <code>a.table</code> to check the corresponding state value.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/aggregation_V.jl#L1-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.FourierV" href="#Ju.FourierV"><code>Ju.FourierV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">FourierV &lt;: AbstractVApproximator{Int}

struct FourierV &lt;: AbstractVApproximator{Int}
    weights::Vector{Float64}
end</code></pre><p>Using Fourier cosine basis to approximate the state value. <code>weights</code> is the featur vector.</p><p>See more details at Section (9.5.2) on Page 211 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/fourier_V.jl#L1-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.FourierV-Tuple{Int64}" href="#Ju.FourierV-Tuple{Int64}"><code>Ju.FourierV</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">FourierV(order::Int)</code></pre><p>By specifying the <code>order</code>, feature vector will be initialized with 0.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/fourier_V.jl#L17-L21">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.LinearV" href="#Ju.LinearV"><code>Ju.LinearV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct LinearV &lt;: AbstractVApproximator{Int}
    features::Array{Float64, 2}
    weights::Vector{Float64}
end</code></pre><p>Using a matrix <code>features</code> to represent each state along with a vector of <code>weights</code>.</p><p>See more details at Section (9.4) on Page 205 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/linear_V.jl#L3-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.PolynomialV" href="#Ju.PolynomialV"><code>Ju.PolynomialV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct PolynomialV &lt;: AbstractVApproximator{Int}
    weights::Vector{Float64}
end</code></pre><p>See more details at Section (9.5.1) on Page 210 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/polynomial_V.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TabularV" href="#Ju.TabularV"><code>Ju.TabularV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct TabularV &lt;: AbstractVApproximator{Int}
    table::Vector{Float64}
end</code></pre><p>Using a <code>table</code> of type <code>Vector{Float64}</code> to record the state values.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/tabular_V.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TilingsV" href="#Ju.TilingsV"><code>Ju.TilingsV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">TilingsV{Tt&lt;:Tiling} &lt;: AbstractVApproximator{Vector{Float64}}
TilingsV(tilings::Vector{Tt}) where Tt&lt;:Tiling</code></pre><p>Using a vector of <code>tilings</code> to encode state. Each tiling has an independent weight.</p><p>See more details at Section (9.5.4) on Page 217 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p><p>See also: <a href="../utilities/#Ju.Tiling"><code>Tiling</code></a>, <a href="#Ju.TilingsQ"><code>TilingsQ</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/tilings.jl#L47-L56">source</a></section><h3><a class="nav-anchor" id="Action-Value-Functions-(Q-functions)-1" href="#Action-Value-Functions-(Q-functions)-1">Action Value Functions (Q functions)</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TabularQ" href="#Ju.TabularQ"><code>Ju.TabularQ</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct TabularQ &lt;: AbstractQApproximator{Int, Int}
    table::Array{Float64, 2}
end</code></pre><p>Using a <code>table</code> of type <code>Array{Float64,2}</code> to record the action value of each state.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/tabular_Q.jl#L3-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TabularQ" href="#Ju.TabularQ"><code>Ju.TabularQ</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">TabularQ(ns::Int, na::Int=1, init::Float64=0.)</code></pre><p>Initial a table of size <code>(ns, na)</code> filled with value of <code>init</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/tabular_Q.jl#L14-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.TilingsQ" href="#Ju.TilingsQ"><code>Ju.TilingsQ</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">TilingsQ{Tt&lt;:Tiling} &lt;: AbstractQApproximator{Vector{Float64}, Int}
TilingsQ(tilings::Vector{Tt}, nactions) where Tt&lt;:Tiling</code></pre><p>The only difference compared to <a href="#Ju.TilingsV"><code>TilingsV</code></a> is that now the weight of each tiling is a matrix.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/approximators/tilings.jl#L78-L83">source</a></section><h2><a class="nav-anchor" id="Environment-Models-1" href="#Environment-Models-1">Environment Models</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DeterministicDistributionModel" href="#Ju.DeterministicDistributionModel"><code>Ju.DeterministicDistributionModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>struct DeterministicDistributionModel &lt;: AbstractDistributionModel       table::Array{Vector{NamedTuple{(:nextstate, :reward, :prob), Tuple{Int, Float64, Float64}}}, 2}    end</p><p>Store all the transformations in the <code>table</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/environment_models/deterministic_distribution_model.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DynamicDistributionModel" href="#Ju.DynamicDistributionModel"><code>Ju.DynamicDistributionModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct DynamicDistributionModel{Tf&lt;:Function} &lt;: AbstractDistributionModel
    f::Tf
    ns::Int
    na::Int
end</code></pre><p>Using a general function <code>f</code> to store the transformations.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/environment_models/dynamic_distribution_model.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.ExperienceSampleModel" href="#Ju.ExperienceSampleModel"><code>Ju.ExperienceSampleModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">  ExperienceSampleModel &lt;: AbstractSampleModel</code></pre><p>Generate a turn sample based on previous experiences.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/environment_models/experience_sample_model.jl#L3-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.PrioritizedSweepingSampleModel" href="#Ju.PrioritizedSweepingSampleModel"><code>Ju.PrioritizedSweepingSampleModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">PrioritizedSweepingSampleModel &lt;: AbstractSampleModel
PrioritizedSweepingSampleModel(θ::Float64=1e-4)</code></pre><p>See more details at Section (8.4) on Page 168 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/environment_models/prioritized_sweeping_sample_model.jl#L3-L8">source</a></section><h2><a class="nav-anchor" id="Policies-1" href="#Policies-1">Policies</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.DeterministicPolicy" href="#Ju.DeterministicPolicy"><code>Ju.DeterministicPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct DeterministicPolicy &lt;: AbstractPolicy
    table::Vector{Int}
    nactions::Int
end</code></pre><p>The action to be adopt is stored in <code>table</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/policies/deterministic_policy.jl#L3-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpsilonGreedyPolicy" href="#Ju.EpsilonGreedyPolicy"><code>Ju.EpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct EpsilonGreedyPolicy &lt;: AbstractPolicy
    table::Vector{Int}
    nactions::Int
    ϵ::Float64
end</code></pre><p>The best actions are stored in the <code>table</code>. However the best action will only be taken at a portion of 1 - ϵ.</p><p>See also: <a href="#Ju.EpsilonGreedySelector"><code>EpsilonGreedySelector</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/policies/epsilon_greedy_policy.jl#L1-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.LinearPolicy" href="#Ju.LinearPolicy"><code>Ju.LinearPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">struct LinearPolicy &lt;: AbstractPolicy
    features::Array{Float64, 3}
    weights::Vector{Float64}
end</code></pre><p>The probability of each action is calculate by <code>features</code> and <code>weights</code> and then normalized by softmax.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/policies/linear_policy.jl#L4-L11">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.RandomPolicy" href="#Ju.RandomPolicy"><code>Ju.RandomPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">RandomPolicy(prob::Array{Float64, 2})
RandomPolicy(prob::Vector{Float64})</code></pre><p>The probability of each action is predefined by <code>prob</code>. If <code>prob</code> is a vector, then all states share the same <code>prob</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/policies/random_policy.jl#L3-L9">source</a></section><h2><a class="nav-anchor" id="Action-Selectors-1" href="#Action-Selectors-1">Action Selectors</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.AlternateSelector" href="#Ju.AlternateSelector"><code>Ju.AlternateSelector</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">AlternateSelector &lt;: AbstractActionSelector</code></pre><p>Used to ensure that all actions are selected alternatively.</p><pre><code class="language-none">AlternateSelector(n::Int)</code></pre><p><code>n::Int</code> means the optional actions are <code>1:n</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/alternate.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.AlternateSelector-Tuple{Any}" href="#Ju.AlternateSelector-Tuple{Any}"><code>Ju.AlternateSelector</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">(s::AlternateSelector)(values::Any)</code></pre><p>Ignore the action <code>values</code>, generate an action alternatively.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; selector = AlternateSelector(3)
AlternateSelector(3, 0)

julia&gt; any_state = 0 # for AlternateSelector, state can be anything

julia&gt; [selector(any_state) for i in 1:10]  # iterate through all actions
10-element Array{Int64,1}:
 1
 2
 3
 1
 2
 3
 1
 2
 3
 1</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/alternate.jl#L21-L47">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpsilonGreedySelector" href="#Ju.EpsilonGreedySelector"><code>Ju.EpsilonGreedySelector</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">EpsilonGreedySelector &lt;: AbstractActionSelector
EpsilonGreedySelector(ϵ)</code></pre><p>The best action is selected for a proportion <span>$1 - \epsilon$</span> and a random action (with uniform probability) is selected for a proportion <span>$\epsilon$</span>.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; selector = EpsilonGreedySelector(0.1)
EpsilonGreedySelector(0.1)

julia&gt; countmap(selector([1,2,1,1]) for _ in 1:1000)
Dict{Any,Int64} with 4 entries:
  4 =&gt; 37
  2 =&gt; 915
  3 =&gt; 22
  1 =&gt; 26</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/epsilon_greedy.jl#L1-L21">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.EpsilonGreedySelector-Tuple{Any}" href="#Ju.EpsilonGreedySelector-Tuple{Any}"><code>Ju.EpsilonGreedySelector</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">(p::EpsilonGreedySelector)(values::AbstractArray)</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>If multiple values with the same maximum value are found. Then a random one will be returned!</p><p><code>NaN</code> will be filtered unless all the values are <code>NaN</code>. In that case, a random one will be returned.</p></div></div></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/epsilon_greedy.jl#L27-L36">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.UpperConfidenceBound" href="#Ju.UpperConfidenceBound"><code>Ju.UpperConfidenceBound</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">UpperConfidenceBound &lt;: AbstractActionSelector
UpperConfidenceBound(na, c=2.0, t=0)</code></pre><p><strong>Arguments</strong></p><ul><li><code>na</code> is the number of actions used to create a internal counter.</li><li><code>t</code> is used to store current time step.</li><li><code>c</code> is used to control the degree of exploration.</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/upper_confidence_bound.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.UpperConfidenceBound-Tuple{AbstractArray}" href="#Ju.UpperConfidenceBound-Tuple{AbstractArray}"><code>Ju.UpperConfidenceBound</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">(ucb::UpperConfidenceBound)(values::AbstractArray)</code></pre><p>Unlike <a href="#Ju.EpsilonGreedySelector"><code>EpsilonGreedySelector</code></a>, uncertaintyies are considered in UCB.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>If multiple values with the same maximum value are found. Then a random one will be returned!</p></div></div><div>\[A_t = \underset{a}{\arg \max} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]\]</div><p>See more details at Section (2.7) on Page 35 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/upper_confidence_bound.jl#L17-L31">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.WeightedSample" href="#Ju.WeightedSample"><code>Ju.WeightedSample</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">WeightedSample &lt;: AbstractActionSelector
WeightedSample()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/weighted_sample.jl#L1-L4">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Ju.WeightedSample-Tuple{AbstractArray}" href="#Ju.WeightedSample-Tuple{AbstractArray}"><code>Ju.WeightedSample</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">(p::WeightedSample)(values::AbstractArray)</code></pre><p>Action <code>values</code> are normalized to have a sum of 1.0 and then used as the probability to sample a random action.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Ju-jl/Ju.jl/blob/01483278f1bbc1d5c26eb0082daf73502a33c93f/src/action_selectors/weighted_sample.jl#L7-L12">source</a></section><footer><hr/><a class="previous" href="../interfaces/"><span class="direction">Previous</span><span class="title">Interfaces</span></a><a class="next" href="../utilities/"><span class="direction">Next</span><span class="title">Utilities</span></a></footer></article></body></html>
