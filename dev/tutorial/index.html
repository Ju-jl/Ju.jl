<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · Ju.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="Ju.jl logo"/></a><h1>Ju.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li class="current"><a class="toctext" href>Tutorial</a><ul class="internal"><li class="toplevel"><a class="toctext" href="#Solving-a-Simple-Random-Walk-Problem-1">Solving a Simple Random Walk Problem</a></li></ul></li><li><a class="toctext" href="../interfaces/">Interfaces</a></li><li><a class="toctext" href="../components/">Components</a></li><li><a class="toctext" href="../utilities/">Utilities</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Tutorial</a></li></ul><a class="edit-page" href="https://github.com/Ju-jl/Ju.jl/blob/master/docs/src/tutorial.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Tutorial</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Introduction-to-Reinforcement-Learning-1" href="#Introduction-to-Reinforcement-Learning-1">Introduction to Reinforcement Learning</a></h1><p>Before showing you a concrete example, let&#39;s review some of the key concepts in RL(Reinforcement Learning).</p><p><img src="../assets/figures/Environment_Agent_Interaction.png" alt="Environment_Agent_Interaction.png"/></p><p>Generally speaking, RL is to learn how to make actions so as to maximize a numerical reward. Two main characters in RL problems are the <strong>agent(s)</strong> and the <strong>environment</strong>. By interacting with the unknown environment, the agent seeks to achieve a goal over time. From the perspective of an agent, it needs to know the observation space and the action space of an environment.</p><ul><li><p><strong>Observation Space</strong></p><p>The <strong>observation</strong> of an environment could be a scalar, a vector, a matrix or a high order tensor(or a combination of them). For example, in video games we can usually use an `Array{Int8,3} to represent the observation at each time. The <strong>Observation Space</strong> tells the agent what kind of input the agent may observe.</p></li><li><p><strong>Action Space</strong></p><p>The <strong>Action Space</strong> defines what <strong>actions</strong> are valid to an environment. Just like observations, actions can also have different dimensions and can be discrete or continuous.</p></li></ul><p>Some of the key concepts of an agent are that:</p><ul><li><p><strong>State</strong></p><p><strong>State</strong> is the agent’s internal representation of the observation from environment. It may or may not be the same with observation.</p></li><li><p><strong>Reward</strong></p><p>After taking an action in the environment, an agent will get a new state accompanied with a numeric reward. Our goal is to maximize some notation of cumulative rewards.</p></li><li><p><strong>Policy</strong></p><p>A policy is used to map a state to an action. It can be deterministic(a hash table) or stochastic(a parameterized classifier/approximator).</p></li><li><p><strong>Experience Buffer</strong> (or <strong>Trajectories</strong>)</p><p>Buffer is used to store the agent’s experiences, including states, actions, rewards and/or some extra_info.</p></li></ul><h1><a class="nav-anchor" id="Solving-a-Simple-Random-Walk-Problem-1" href="#Solving-a-Simple-Random-Walk-Problem-1">Solving a Simple Random Walk Problem</a></h1><p><img src="../assets/figures/simple_random_walk.png" alt="simple_random_walk.png"/></p><p>Suppose our friend Eve starts at the position of 3 in the above figure. It can either move left or right randomly at each time step. If it meets the dog at position 7 then receives a reward of 3 and if it meets the flower at position 1 then receives a reward of 1. In all other cases, the reward is 0. Now consider that Eve choose to move to each direction with the same probability(the <strong>policy</strong>). And we would like to estimate the value of each position. So that we can further improve the policy.</p><p>First, let&#39;s initial the environment and the policy.</p><pre><code class="language-julia">julia&gt; using Ju

julia&gt; using Random

julia&gt; Random.seed!(123);  # to ensure that you get the same result as it is documented here

julia&gt; policy = RandomPolicy([0.5, 0.5])
RandomPolicy{1}([0.5, 0.5])

julia&gt; env = SimpleRandomWalkEnv()
SimpleRandomWalkEnv(7, 3, 3, [-1, 1])

julia&gt; env(sample(actionspace(env)))
(observation = 2, reward = 0.0, isdone = false, extra_info = ())

julia&gt; reset!(env)
(observation = 3, isdone = false)</code></pre><p>Since we want to estimate the value of each position, we choose an approximator of type <code>TabularV</code> to store and update our estimations(the value function).</p><pre><code class="language-julia">julia&gt; approximator = TabularV(length(observationspace(env)));</code></pre><p>Then we need to decide how to update our estimations. There are many different kinds of algorithms. Here we use one of the simplest algorithms, the <code>MonteCarloLearner</code>. The basic idea behind <code>MonteCarloLearner</code> is that, we apply a policy until the end of an episode and then update the estimation of each state we have encountered by using following up rewards.</p><pre><code class="language-julia">julia&gt; learner = MonteCarloLearner(approximator, policy, 0.9, 0.1);</code></pre><p>Here we set the discount rate to <code>0.9</code> and the step size of updating to <code>0.1</code>.</p><p>Then we use a <code>EpisodeSARDBuffer</code> to store the <em>S</em>tate, <em>A</em>ction, <em>R</em>eward, is<em>D</em>one at each time step.</p><pre><code class="language-julia">julia&gt; buffer = EpisodeSARDBuffer();</code></pre><p>Combining all the components above, now we have our agent.</p><pre><code class="language-julia">julia&gt; agent = Agent(learner, buffer);</code></pre><p>Finally, we can train our agent!</p><pre><code class="language-julia">julia&gt; train!(env, agent);</code></pre><p>Every time we call <code>train!(env, agent)</code>, the <code>agent</code> will generate an action according to its policy and feed it into the <code>env</code>. Then the <code>env</code> will consume the action and return a reward, isdone and next state.</p><pre><code class="language-julia">julia&gt; agent.buffer
1-element EpisodeTurnBuffer{(:state, :action, :reward, :isdone),Tuple{Int64,Int64,Float64,Bool},NamedTuple{(:state, :action, :reward, :isdone),Tuple{Array{Int64,1},Array{Int64,1},Array{Float64,1},Array{Bool,1}}}}:
 (state = 3, action = 2, reward = 0.0, isdone = false)</code></pre><p>It will be too verbose to train our agent step by step. Fortunately, <code>train!</code> can accept an optional argument named <code>callbacks</code>. We can force the training to stop at some condition, like stop at the end of an episode.</p><pre><code class="language-julia">julia&gt; callbacks=(stop_at_episode(1),);

julia&gt; train!(env, agent;callbacks=callbacks);

julia&gt; agent.buffer
8-element EpisodeTurnBuffer{(:state, :action, :reward, :isdone),Tuple{Int64,Int64,Float64,Bool},NamedTuple{(:state, :action, :reward, :isdone),Tuple{Array{Int64,1},Array{Int64,1},Array{Float64,1},Array{Bool,1}}}}:
 (state = 3, action = 2, reward = 0.0, isdone = false)
 (state = 4, action = 2, reward = 0.0, isdone = false)
 (state = 3, action = 1, reward = 0.0, isdone = false)
 (state = 2, action = 2, reward = 0.0, isdone = false)
 (state = 3, action = 2, reward = 0.0, isdone = false)
 (state = 4, action = 1, reward = 0.0, isdone = false)
 (state = 3, action = 1, reward = 0.0, isdone = false)
 (state = 2, action = 1, reward = 1.0, isdone = true)

julia&gt; agent.learner.approximator
TabularV([0.0, 0.06561, 0.0478297, 0.0531441, 0.0, 0.0, 0.0])</code></pre><p>As you can see, the <code>agent.learner.approximator</code> has been updated a little. Then we increase the number of training episodes to 1000.</p><pre><code class="language-julia">julia&gt; callbacks=(stop_at_episode(1000),);

julia&gt; train!(env, agent;callbacks=callbacks);
Progress: 100%|█████████████████████████████████████████| Time: 0:00:03
  episode:  1000

julia&gt; agent.learner.approximator
TabularV([0.0, 0.896335, 0.851624, 1.00933, 1.41548, 2.13538, 0.0])</code></pre><p>Now we get our estimation of each position under the discount rate of <code>0.9</code>.</p><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../interfaces/"><span class="direction">Next</span><span class="title">Interfaces</span></a></footer></article></body></html>
